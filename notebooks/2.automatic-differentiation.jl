### A Pluto.jl notebook ###
# v0.12.21

using Markdown
using InteractiveUtils

# ╔═╡ a8439cf0-8eab-11eb-3d6b-a3d65e46a635
md"""
# Automatic Differentiation (AD)

Automatic differentiation is a method to evaluate the derivatives of a given program, you may heard this in context of deep learning as back-propagation. In this section, we will introduce the automatic differentiation technique in a more systematic aspect
"""

# ╔═╡ f4228cb2-8eab-11eb-1b98-fbb30f8eeac3
md"""
# Forward Mode AD

The forward mode AD is the simplest way of implementing an AD system
"""

# ╔═╡ 13c539e8-8eac-11eb-35b0-61108d5f3c5f
md"""
# Reverse Mode AD

For large number of parameters, reverse mode is more efficient. This method has been re-discovered many times in history in different fields, you may hear it called back-propagation in context of deep learning.
"""

# ╔═╡ 6cf358d8-8eac-11eb-34ef-43bd6b60c4d5
md"""
## Machine Learning System/Framework
"""

# ╔═╡ 3fc036f8-8eac-11eb-15dc-017c0d5237bc
md"""
## Operator Overloading AD
"""

# ╔═╡ 46ad0fa2-8eac-11eb-28df-4f2c23f8d3ff
md"""
## Source Code Transformation AD
"""

# ╔═╡ 84e5fe6e-8eac-11eb-04f8-cf7499b2c218
md"""
## Other type of AD engines
"""

# ╔═╡ 4c23d3bc-8eac-11eb-2870-a37dfc123bc5
md"""
## Differentiable Programming
"""

# ╔═╡ 5c03472c-8eac-11eb-0072-b50fe89704fc
md"""
## Implement Your own AD engine in ONE day
"""

# ╔═╡ Cell order:
# ╟─a8439cf0-8eab-11eb-3d6b-a3d65e46a635
# ╟─f4228cb2-8eab-11eb-1b98-fbb30f8eeac3
# ╟─13c539e8-8eac-11eb-35b0-61108d5f3c5f
# ╠═6cf358d8-8eac-11eb-34ef-43bd6b60c4d5
# ╠═3fc036f8-8eac-11eb-15dc-017c0d5237bc
# ╠═46ad0fa2-8eac-11eb-28df-4f2c23f8d3ff
# ╠═84e5fe6e-8eac-11eb-04f8-cf7499b2c218
# ╠═4c23d3bc-8eac-11eb-2870-a37dfc123bc5
# ╠═5c03472c-8eac-11eb-0072-b50fe89704fc
